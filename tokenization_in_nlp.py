# -*- coding: utf-8 -*-
"""Tokenization_in_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eWiieaYOeKrdyFrpyHTMn-iQoDaUw-sh

# WHITE SPACE TOKENIZATION

The simplest way is to use python split function to tokenize text based on whitespace
"""

tweet_text1 = "Just finished my workout üí™ Feeling energized and ready to tackle the day! #fitness #motivation."

tweet_text1.split()

"""We can observe in the above output that the punctuations have not been separated from the word, for example:  'day !', '#fitness', '#motivation.'

Splitting the sentences using delimeter
"""

tweet_text2 = "Just finished a great workout! üí™ Feeling energized and ready to take on the day. üåü Can't wait for dinner tonight with friends. üçï #fitness #friends #goodtimes"

tweet_text2.split('.')

"""### We can clearly see that the split() function has not done a good job splitting the text into sentences"""

#*****************************************************************************************************************************************************************

"""# NLTK python library"""

# !pip install nltk
import nltk
nltk.download('punkt')

"""#### Word Tokenization

"""

from nltk.tokenize import word_tokenize

print(word_tokenize(tweet_text1))
print()
print(f"Total number of tokens: {len(word_tokenize(tweet_text1))}")

"""#### WordPunct Tokenization

"""

from nltk.tokenize import WordPunctTokenizer

word_tokenizer = WordPunctTokenizer()
print(word_tokenizer.tokenize(tweet_text1))
print()
print(f"Total number of tokens: {len(word_tokenizer.tokenize(tweet_text1))}")

"""#### Sentence Tokenization"""

from nltk.tokenize import sent_tokenize

print(sent_tokenize(tweet_text2))
print()
print(f"Total number of tokens: {len(sent_tokenize(tweet_text1))}")

"""#### RegEx Tokenization"""

from nltk.tokenize import regexp_tokenize

regexp_tokenize(tweet_text1, "[\w']+") # you can observe the punctuations / special characters are excluded

"""#### TreebankWord tokenization

"""

from nltk.tokenize import TreebankWordTokenizer

tbword_tokenizer = TreebankWordTokenizer()
print(tbword_tokenizer.tokenize(tweet_text2))
print()
print(f"Total number of  tokens: {len(tbword_tokenizer.tokenize(tweet_text2))}")

##Output: 'day.' , "n't" is not tokenized
## It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token.
## Rules for Treebank : https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank

"""#### Tweet Tokenizer"""

from nltk.tokenize import TweetTokenizer

tweet_tokenizer = TweetTokenizer()
print(tweet_tokenizer.tokenize(tweet_text2))
print()
print(f"Total number of  tokens: {len(tweet_tokenizer.tokenize(tweet_text2))}")

"""#### MWET Tokenization"""

from nltk.tokenize import MWETokenizer

mwet_tokenizer = MWETokenizer()
print(mwet_tokenizer.tokenize(tweet_text1))
print()
print(f"Total number of  tokens: {len(mwet_tokenizer.tokenize(tweet_text1))}")

# Initialize MWETokenizer
mwetokenizer = MWETokenizer([('Feeling', 'energized'), ('dinner', 'tonight')])

# Text to tokenize
text = "Just finished a great workout! Feeling energized and ready for dinner tonight with friends."
tokens = nltk.word_tokenize(text)

# Apply MWETokenizer
mwet_tokens = mwetokenizer.tokenize(tokens)

print(mwet_tokens)

"""A Multi-Word Expression Tokenizer (MWET) treat certain sequences of words as single tokens rather than individual words. This is particularly useful when dealing with multi-word expressions or phrases that have a specific meaning when they appear together.

The MWET tokenizer allows you to specify which sequences of words should be treated as single tokens. For example, in the above example "Feeling energized" you might want to treat "Feeling_energized" as a single token rather than two separate tokens.

This will preserve the meaning of multi-word expressions during text processing tasks such as part-of-speech tagging, named entity recognition, and machine translation. By treating these expressions as single tokens, the tokenizer ensures that their semantic integrity is maintained.

#### TextBlob

TextBlob is a Python library for processing textual data, primarily designed for tasks in NLP. It provides a simple API for common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.
"""

# ! pip install -U textblob
# ! python3 -m textblob.download_corpora

from textblob import TextBlob

# 1. Sentiment Analysis
text1 = "TextBlob is great for performing sentiment analysis!"
text2 = "Life sucks!"
blob1 = TextBlob(text1)
blob2 = TextBlob(text2)
print("Sentiment Analysis:")
print("Polarity:", blob1.sentiment.polarity)  # Positive sentiment
print()
print("Polarity:", blob2.sentiment.polarity)  # Negative sentiment

# 2. Part-of-Speech Tagging
print("\nPart-of-Speech Tagging:")
print(blob1.tags)

# 3. Noun Phrase Extraction
print("\nNoun Phrase Extraction:")
print(blob1.noun_phrases)

# 4. Tokenization
print("\nTokenization:")
print(blob1.words)  # Tokenize into words
print(blob1.sentences)  # Tokenize into sentences

# 5. Word Inflection and Lemmatization
print("\nWord Inflection and Lemmatization:")
print("Plural of 'cat':", blob1.words[1].pluralize())
print("Singular of 'dogs':", blob1.words[-1].singularize())
print("Lemmatization of 'running':", blob1.words[3].lemmatize('v'))

# 6. Language Translation
print("\nLanguage Translation:")
blob = TextBlob("Hello!")
print(blob.translate(from_lang='en', to='fr')) # use both from_lang and to. The code might throw error.

# 7. Spelling Correction
print("\nSpelling Correction:")
text_with_typo = "The quick brown fox jumps ovri the lazy dog."
corrected_blob = TextBlob(text_with_typo).correct()
print(corrected_blob)

# 8. n-grams
blob1.ngrams(n=2)

# 9. Word counts
blob1.words.count('for')

# 10. Parse the text
blob1.parse()

blob1.upper()

blob2.words.pluralize()

from textblob import Word
from textblob.wordnet import VERB
word = Word("sentiment")
print(word.synsets)
print()
print(word.definitions)

word2 = Word('Feelings')
word2.synsets[0]

from textblob.wordnet import Synset
sent = Synset("sentiment.n.01")
feel = Synset("feelings.n.01")
sent.path_similarity(feel)

"""#### Spacy Tokenizer"""

# ! pip install spacy
# ! python -m spacy download en_core_web_sm

import spacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")
text = "SpaCy is a great tool for natural language processing!"
doc = nlp(text)

# Print the tokens
print("Tokenization:")
for token in doc:
    print(token.text)

text = "SpaCy is a great tool for natural language processing! It can tokenize text and perform various NLP tasks. Give it a try."
doc = nlp(text)
sentences = list(doc.sents)

# Print the segmented sentences
print("Segmented Sentences:")
for i, sentence in enumerate(sentences, 1):
    print(f"Sentence {i}: {sentence}")

"""#### Gensim Tokenizer"""

#SYNTAX:  gensim.utils.tokenize(text, lowercase=True, deacc=False, errors='strict', to_lower=False, lower=False)
from gensim.utils import simple_preprocess

text = "Gensim is a Python library for topic modeling and document similarity analysis."
tokens = nltk.word_tokenize(text)
processed_tokens = simple_preprocess(" ".join(tokens))

print("Tokenization using NLTK and preprocessing using Gensim:")
print(processed_tokens)

"""#### Tokenization with Keras"""

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.text import text_to_word_sequence

texts = "Keras is a high-level neural networks API running on top of TensorFlow."

tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
list_tokens = text_to_word_sequence(texts)
list_tokens

#####################################################################################END#####################################################################################