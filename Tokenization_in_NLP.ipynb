{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WHITE SPACE TOKENIZATION\n",
        "\n",
        "The simplest way is to use python split function to tokenize text based on whitespace"
      ],
      "metadata": {
        "id": "Kikr97bqDF2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QgnJA-86CJbM"
      },
      "outputs": [],
      "source": [
        "tweet_text1 = \"Just finished my workout üí™ Feeling energized and ready to tackle the day! #fitness #motivation.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_text1.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0O8XPqODjhd",
        "outputId": "bb5d65e1-a284-4768-e0e1-f9e2be878d7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'finished',\n",
              " 'my',\n",
              " 'workout',\n",
              " 'üí™',\n",
              " 'Feeling',\n",
              " 'energized',\n",
              " 'and',\n",
              " 'ready',\n",
              " 'to',\n",
              " 'tackle',\n",
              " 'the',\n",
              " 'day!',\n",
              " '#fitness',\n",
              " '#motivation.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We can observe in the above output that the punctuations have not been separated from the word, for example:  'day !', '#fitness', '#motivation.'"
      ],
      "metadata": {
        "id": "8wtGIQ69C3ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the sentences using delimeter"
      ],
      "metadata": {
        "id": "JKEpu_YoEtEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_text2 = \"Just finished a great workout! üí™ Feeling energized and ready to take on the day. üåü Can't wait for dinner tonight with friends. üçï #fitness #friends #goodtimes\"\n",
        "\n",
        "tweet_text2.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu5zK5OlECEk",
        "outputId": "e789311e-e5d4-454a-f2c1-18ab4031128c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just finished a great workout! üí™ Feeling energized and ready to take on the day',\n",
              " \" üåü Can't wait for dinner tonight with friends\",\n",
              " ' üçï #fitness #friends #goodtimes']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can clearly see that the split() function has not done a good job splitting the text into sentences"
      ],
      "metadata": {
        "id": "ZNHBew8uE_7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#*****************************************************************************************************************************************************************"
      ],
      "metadata": {
        "id": "uMOUoTdlEmaB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK python library"
      ],
      "metadata": {
        "id": "FllCD5CfFL1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNhuM3NLFDd4",
        "outputId": "6e87ad94-1ef4-4ac6-93e4-719c8a7d0856"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word Tokenization\n"
      ],
      "metadata": {
        "id": "oMqqrdaUFwV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(word_tokenize(tweet_text1))\n",
        "print()\n",
        "print(f\"Total number of tokens: {len(word_tokenize(tweet_text1))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdVkr4ErFZ-w",
        "outputId": "bfa742e5-bea7-424f-e5cc-48772318b794"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'finished', 'my', 'workout', 'üí™', 'Feeling', 'energized', 'and', 'ready', 'to', 'tackle', 'the', 'day', '!', '#', 'fitness', '#', 'motivation', '.']\n",
            "\n",
            "Total number of tokens: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### WordPunct Tokenization\n"
      ],
      "metadata": {
        "id": "cXLHpm1JF-jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "word_tokenizer = WordPunctTokenizer()\n",
        "print(word_tokenizer.tokenize(tweet_text1))\n",
        "print()\n",
        "print(f\"Total number of tokens: {len(word_tokenizer.tokenize(tweet_text1))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL6p1URRFsuC",
        "outputId": "b50dda51-dc0f-4bd2-bbbe-1cf5f937be45"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'finished', 'my', 'workout', 'üí™', 'Feeling', 'energized', 'and', 'ready', 'to', 'tackle', 'the', 'day', '!', '#', 'fitness', '#', 'motivation', '.']\n",
            "\n",
            "Total number of tokens: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sentence Tokenization"
      ],
      "metadata": {
        "id": "-VaWRN_jGqDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "print(sent_tokenize(tweet_text2))\n",
        "print()\n",
        "print(f\"Total number of tokens: {len(sent_tokenize(tweet_text1))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t6fNqDCGLuM",
        "outputId": "8465d64f-a973-40c8-a75b-70d1598ff7e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just finished a great workout!', 'üí™ Feeling energized and ready to take on the day.', \"üåü Can't wait for dinner tonight with friends.\", 'üçï #fitness #friends #goodtimes']\n",
            "\n",
            "Total number of tokens: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RegEx Tokenization"
      ],
      "metadata": {
        "id": "iC9lZhFiG2HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "regexp_tokenize(tweet_text1, \"[\\w']+\") # you can observe the punctuations / special characters are excluded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5PMIp3DGsYQ",
        "outputId": "e055bac9-d0e4-4331-840f-75b4ff29c802"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'finished',\n",
              " 'my',\n",
              " 'workout',\n",
              " 'Feeling',\n",
              " 'energized',\n",
              " 'and',\n",
              " 'ready',\n",
              " 'to',\n",
              " 'tackle',\n",
              " 'the',\n",
              " 'day',\n",
              " 'fitness',\n",
              " 'motivation']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TreebankWord tokenization\n"
      ],
      "metadata": {
        "id": "K6OMiU6bHhkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tbword_tokenizer = TreebankWordTokenizer()\n",
        "print(tbword_tokenizer.tokenize(tweet_text2))\n",
        "print()\n",
        "print(f\"Total number of  tokens: {len(tbword_tokenizer.tokenize(tweet_text2))}\")\n",
        "\n",
        "##Output: 'day.' , \"n't\" is not tokenized\n",
        "## It separates phrase-terminating punctuation like (?!.;,) from adjacent tokens and retains decimal numbers as a single token.\n",
        "## Rules for Treebank : https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.treebank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNIeq_5zG8Z7",
        "outputId": "9926942b-d132-4a46-e9ba-037d578bfdd7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'finished', 'a', 'great', 'workout', '!', 'üí™', 'Feeling', 'energized', 'and', 'ready', 'to', 'take', 'on', 'the', 'day.', 'üåü', 'Ca', \"n't\", 'wait', 'for', 'dinner', 'tonight', 'with', 'friends.', 'üçï', '#', 'fitness', '#', 'friends', '#', 'goodtimes']\n",
            "Total number of  tokens: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tweet Tokenizer"
      ],
      "metadata": {
        "id": "eYi9pUX1IK9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(tweet_tokenizer.tokenize(tweet_text2))\n",
        "print()\n",
        "print(f\"Total number of  tokens: {len(tweet_tokenizer.tokenize(tweet_text2))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BULIrj1Hd4K",
        "outputId": "880e37a4-1d5c-4578-a2d5-7128942170c1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'finished', 'a', 'great', 'workout', '!', 'üí™', 'Feeling', 'energized', 'and', 'ready', 'to', 'take', 'on', 'the', 'day', '.', 'üåü', \"Can't\", 'wait', 'for', 'dinner', 'tonight', 'with', 'friends', '.', 'üçï', '#fitness', '#friends', '#goodtimes']\n",
            "\n",
            "Total number of  tokens: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MWET Tokenization"
      ],
      "metadata": {
        "id": "nkUbyFfzJTYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwet_tokenizer = MWETokenizer()\n",
        "print(mwet_tokenizer.tokenize(tweet_text1))\n",
        "print()\n",
        "print(f\"Total number of  tokens: {len(mwet_tokenizer.tokenize(tweet_text1))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvfBGUyaIW4A",
        "outputId": "7ceb7dc8-51dd-4dca-f19b-e34eb15d9bc9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['J', 'u', 's', 't', ' ', 'f', 'i', 'n', 'i', 's', 'h', 'e', 'd', ' ', 'm', 'y', ' ', 'w', 'o', 'r', 'k', 'o', 'u', 't', ' ', 'üí™', ' ', 'F', 'e', 'e', 'l', 'i', 'n', 'g', ' ', 'e', 'n', 'e', 'r', 'g', 'i', 'z', 'e', 'd', ' ', 'a', 'n', 'd', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 't', 'a', 'c', 'k', 'l', 'e', ' ', 't', 'h', 'e', ' ', 'd', 'a', 'y', '!', ' ', '#', 'f', 'i', 't', 'n', 'e', 's', 's', ' ', '#', 'm', 'o', 't', 'i', 'v', 'a', 't', 'i', 'o', 'n', '.']\n",
            "\n",
            "Total number of  tokens: 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MWETokenizer\n",
        "mwetokenizer = MWETokenizer([('Feeling', 'energized'), ('dinner', 'tonight')])\n",
        "\n",
        "# Text to tokenize\n",
        "text = \"Just finished a great workout! Feeling energized and ready for dinner tonight with friends.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Apply MWETokenizer\n",
        "mwet_tokens = mwetokenizer.tokenize(tokens)\n",
        "\n",
        "print(mwet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2kkw7_WIo9U",
        "outputId": "b54a2ecd-1b87-42a3-9aa6-75dbf169774a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'finished', 'a', 'great', 'workout', '!', 'Feeling_energized', 'and', 'ready', 'for', 'dinner_tonight', 'with', 'friends', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Multi-Word Expression Tokenizer (MWET) treat certain sequences of words as single tokens rather than individual words. This is particularly useful when dealing with multi-word expressions or phrases that have a specific meaning when they appear together.\n",
        "\n",
        "The MWET tokenizer allows you to specify which sequences of words should be treated as single tokens. For example, in the above example \"Feeling energized\" you might want to treat \"Feeling_energized\" as a single token rather than two separate tokens.\n",
        "\n",
        "This will preserve the meaning of multi-word expressions during text processing tasks such as part-of-speech tagging, named entity recognition, and machine translation. By treating these expressions as single tokens, the tokenizer ensures that their semantic integrity is maintained.\n"
      ],
      "metadata": {
        "id": "Q183iLCFJWrC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TextBlob"
      ],
      "metadata": {
        "id": "0l0kmtqrKDws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob is a Python library for processing textual data, primarily designed for tasks in NLP. It provides a simple API for common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
      ],
      "metadata": {
        "id": "47IRO6BcKGwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -U textblob\n",
        "# ! python3 -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "hDodASzlKGZu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# 1. Sentiment Analysis\n",
        "text1 = \"TextBlob is great for performing sentiment analysis!\"\n",
        "text2 = \"Life sucks!\"\n",
        "blob1 = TextBlob(text1)\n",
        "blob2 = TextBlob(text2)\n",
        "print(\"Sentiment Analysis:\")\n",
        "print(\"Polarity:\", blob1.sentiment.polarity)  # Positive sentiment\n",
        "print()\n",
        "print(\"Polarity:\", blob2.sentiment.polarity)  # Negative sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozn8eG8lI-fI",
        "outputId": "afb20afe-010d-4404-d300-195acef359e7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis:\n",
            "Polarity: 1.0\n",
            "\n",
            "Polarity: -0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "print(\"\\nPart-of-Speech Tagging:\")\n",
        "print(blob1.tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dLMiCRYKWE4",
        "outputId": "a0340828-4d02-45bf-a6e3-5e3df9a586b0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Part-of-Speech Tagging:\n",
            "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('great', 'JJ'), ('for', 'IN'), ('performing', 'VBG'), ('sentiment', 'JJ'), ('analysis', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Noun Phrase Extraction\n",
        "print(\"\\nNoun Phrase Extraction:\")\n",
        "print(blob1.noun_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olcOMo8gK75J",
        "outputId": "8fd39c26-fc62-467d-b67b-262ef1378b7c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Noun Phrase Extraction:\n",
            "['textblob', 'sentiment analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenization\n",
        "print(\"\\nTokenization:\")\n",
        "print(blob1.words)  # Tokenize into words\n",
        "print(blob1.sentences)  # Tokenize into sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKBqFmczK_DN",
        "outputId": "0c7331e4-3de0-4796-cc0c-47cea8f9eff4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization:\n",
            "['TextBlob', 'is', 'great', 'for', 'performing', 'sentiment', 'analysis']\n",
            "[Sentence(\"TextBlob is great for performing sentiment analysis!\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Word Inflection and Lemmatization\n",
        "print(\"\\nWord Inflection and Lemmatization:\")\n",
        "print(\"Plural of 'cat':\", blob1.words[1].pluralize())\n",
        "print(\"Singular of 'dogs':\", blob1.words[-1].singularize())\n",
        "print(\"Lemmatization of 'running':\", blob1.words[3].lemmatize('v'))"
      ],
      "metadata": {
        "id": "DoeTBuEfLIac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Language Translation\n",
        "print(\"\\nLanguage Translation:\")\n",
        "blob = TextBlob(\"Hello!\")\n",
        "print(blob.translate(from_lang='en', to='fr')) # use both from_lang and to. The code might throw error."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOpZbQwNLNCp",
        "outputId": "1d08d83f-d481-4fc8-ce2e-48b84f09d889"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Language Translation:\n",
            "Bonjour!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Spelling Correction\n",
        "print(\"\\nSpelling Correction:\")\n",
        "text_with_typo = \"The quick brown fox jumps ovri the lazy dog.\"\n",
        "corrected_blob = TextBlob(text_with_typo).correct()\n",
        "print(corrected_blob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1BfEvbFLO6S",
        "outputId": "f2bb3a82-f735-4023-9084-8566fcb04463"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spelling Correction:\n",
            "The quick brown fox jumps or the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. n-grams\n",
        "blob1.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwDdk6iNLS4W",
        "outputId": "0f75375d-4877-4848-a8cd-5111de4c636b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['TextBlob', 'is']),\n",
              " WordList(['is', 'great']),\n",
              " WordList(['great', 'for']),\n",
              " WordList(['for', 'performing']),\n",
              " WordList(['performing', 'sentiment']),\n",
              " WordList(['sentiment', 'analysis'])]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Word counts\n",
        "blob1.words.count('for')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GB10zbiOoPh",
        "outputId": "2bbe9176-7d7f-4b78-b75a-6f6816ab5b21"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Parse the text\n",
        "blob1.parse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9Q6VG3qEO1ZR",
        "outputId": "3f8e18ce-081f-4086-d087-10a8a1f37497"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TextBlob/NN/B-NP/O is/VBZ/B-VP/O great/JJ/B-ADJP/O for/IN/B-PP/B-PNP performing/VBG/B-VP/I-PNP sentiment/NN/B-NP/I-PNP analysis/NN/I-NP/I-PNP !/./O/O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob1.upper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HElldOFiPFLk",
        "outputId": "7aefd18a-67c9-4f12-c694-0cceb577382e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"TEXTBLOB IS GREAT FOR PERFORMING SENTIMENT ANALYSIS!\")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob2.words.pluralize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA4tK66IPMzU",
        "outputId": "feae8517-8de7-4222-d3eb-e934c41c0846"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Lifes', 'suckss'])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "from textblob.wordnet import VERB\n",
        "word = Word(\"sentiment\")\n",
        "print(word.synsets)\n",
        "print()\n",
        "print(word.definitions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzV5pnU9Pbkh",
        "outputId": "5733de23-2e59-4caa-c755-1c8b4b1d68e1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('sentiment.n.01'), Synset('opinion.n.01')]\n",
            "\n",
            "['tender, romantic, or nostalgic feeling or emotion', 'a personal belief or judgment that is not founded on proof or certainty']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2 = Word('Feelings')\n",
        "word2.synsets[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v28I-DuUPnBs",
        "outputId": "a44bf112-76ac-4ac7-8686-da221ab2f3e2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('feelings.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob.wordnet import Synset\n",
        "sent = Synset(\"sentiment.n.01\")\n",
        "feel = Synset(\"feelings.n.01\")\n",
        "sent.path_similarity(feel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLBPIRDgP688",
        "outputId": "f8ff1b3b-c2f6-4e11-86dc-f5a39a26a8b9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spacy Tokenizer"
      ],
      "metadata": {
        "id": "a9n9ZMi2VJAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install spacy\n",
        "# ! python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "_FV52gL7QJBV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"SpaCy is a great tool for natural language processing!\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Tokenization:\")\n",
        "for token in doc:\n",
        "    print(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGujaNeQWC2m",
        "outputId": "e92b38dc-d42b-469d-8fa0-104658125e51"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "SpaCy\n",
            "is\n",
            "a\n",
            "great\n",
            "tool\n",
            "for\n",
            "natural\n",
            "language\n",
            "processing\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"SpaCy is a great tool for natural language processing! It can tokenize text and perform various NLP tasks. Give it a try.\"\n",
        "doc = nlp(text)\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "# Print the segmented sentences\n",
        "print(\"Segmented Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JREpUIdWUcT",
        "outputId": "a06d657f-a5d2-4f6d-b01a-dea914ffe94b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentences:\n",
            "Sentence 1: SpaCy is a great tool for natural language processing!\n",
            "Sentence 2: It can tokenize text and perform various NLP tasks.\n",
            "Sentence 3: Give it a try.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gensim Tokenizer"
      ],
      "metadata": {
        "id": "6QUM1X5cYC-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SYNTAX:  gensim.utils.tokenize(text, lowercase=True, deacc=False, errors='strict', to_lower=False, lower=False)\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "text = \"Gensim is a Python library for topic modeling and document similarity analysis.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "processed_tokens = simple_preprocess(\" \".join(tokens))\n",
        "\n",
        "print(\"Tokenization using NLTK and preprocessing using Gensim:\")\n",
        "print(processed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6EdUk8RXP6N",
        "outputId": "b3e12482-5af5-4548-b10f-25eec62a3378"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization using NLTK and preprocessing using Gensim:\n",
            "['gensim', 'is', 'python', 'library', 'for', 'topic', 'modeling', 'and', 'document', 'similarity', 'analysis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization with Keras"
      ],
      "metadata": {
        "id": "JLJ6mhpiYQw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "texts = \"Keras is a high-level neural networks API running on top of TensorFlow.\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "list_tokens = text_to_word_sequence(texts)\n",
        "list_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFjw-KsvYHsu",
        "outputId": "1ed7e73c-3dd1-40aa-c118-072a2a35fcd6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keras',\n",
              " 'is',\n",
              " 'a',\n",
              " 'high',\n",
              " 'level',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'api',\n",
              " 'running',\n",
              " 'on',\n",
              " 'top',\n",
              " 'of',\n",
              " 'tensorflow']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################################END#####################################################################################"
      ],
      "metadata": {
        "id": "proCxOdTYgq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}